{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116 articles in section la-vista.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "Scraping completed. Total articles written: 116\n"
     ]
    }
   ],
   "source": [
    "# Define the keywords of interest\n",
    "keywords = [\n",
    "    \"Asian\", \"hispanic\", \"latinx\", \"diversity\", \"equity\", \"inclusion\", \"equality\", \n",
    "    \"poc\", \"lgbtqia\", \"queer\", \"middle eastern\", \"black\", \"gay\", \"transgender\", \n",
    "    \"bisexual\", \"lesbian\", \"pansexual\", \"asexual\", \"homosexual\", \"nonbinary\", \n",
    "    \"intersex\", \"aromantic\", \"cisgender\", \"coming out\", \"genderfluid\", \"privilege\", \n",
    "    \"pronouns\", \"undocumented\", \"neurodiversity\", \"neurodivergent\", \"disability\", \n",
    "    \"microaggression\", \"identity\", \"discrimination\", \"culture\", \"misgendering\"\n",
    "]\n",
    "\n",
    "# Function to fetch the HTML content of a URL\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract article URLs from a page\n",
    "def extract_article_urls(page_html):\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    article_tags = soup.find_all('div', class_='category-page-post-text')\n",
    "    article_urls = [tag.find('a')['href'] for tag in article_tags if tag.find('a')]\n",
    "    return list(set(article_urls))\n",
    "\n",
    "def clean_content(content):\n",
    "    marker = \"Print Comments are closed.\"\n",
    "    if (idx := content.find(marker)) != -1:\n",
    "        content = content[:idx]\n",
    "    \n",
    "    # Replace \"â€œ\" with opening quotes and \"â€\" with closing quotes\n",
    "    content = content.replace(\"â€œ\", \"\\\"\").replace(\"â€™\", \"\\'\").replace(\"â€\", \"\\\"\").replace(\"Â\", \"\")\n",
    "    return content\n",
    "\n",
    "# Function to extract article content\n",
    "def extract_article_content(article_html):\n",
    "    soup = BeautifulSoup(article_html, 'html.parser')\n",
    "    title_tag = soup.find('title')\n",
    "    date_tag = soup.find('div', class_='single-post-byline')\n",
    "    title = title_tag.text.strip() if title_tag else 'No title found'\n",
    "    date_info = date_tag.text.strip() if date_tag else 'No date found'\n",
    "    \n",
    "    date, time, authors = parse_date_info(date_info)\n",
    "    \n",
    "    content = ' '.join([p.text for p in soup.find_all('p')])\n",
    "    content = clean_content(content)\n",
    "    return title, date, time, authors, content\n",
    "\n",
    "# Function to parse date information\n",
    "def parse_date_info(date_info):\n",
    "    date_time_author_pattern = re.compile(r\"([A-Za-z]+\\s\\d{1,2},\\s\\d{4})\\s+at\\s+(\\d{1,2}:\\d{2}\\s[apm]{2})\\s+by\\s+(.+)\")\n",
    "    match = date_time_author_pattern.search(date_info)\n",
    "    \n",
    "    if match:\n",
    "        date = match.group(1)\n",
    "        time = match.group(2)\n",
    "        authors_str = match.group(3)\n",
    "        authors = [author.strip() for author in re.split(r',\\s*|\\band\\b', authors_str)]\n",
    "        return date, time, authors\n",
    "    else:\n",
    "        return 'No date found', 'No time found', ['No author found']\n",
    "\n",
    "# Function to check if the article content contains any of the keywords\n",
    "def contains_keywords(content, keywords):\n",
    "    content_lower = content.lower()\n",
    "    for keyword in keywords:\n",
    "        if re.search(r'\\b' + re.escape(keyword.lower()) + r'\\b', content_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to get all article URLs by paginating\n",
    "def get_all_article_urls(base_url, max_pages=45):\n",
    "    page = 1\n",
    "    all_article_urls = []\n",
    "    while page <= max_pages:\n",
    "        page_url = f\"{base_url}page/{page}/\"\n",
    "        if page % 10 == 0:\n",
    "            print(f\"Fetching {page_url}...\")\n",
    "        page_html = fetch_html(page_url)\n",
    "        if page_html:\n",
    "            article_urls = extract_article_urls(page_html)\n",
    "            if not article_urls:\n",
    "                break  # No more articles found, exit the loop\n",
    "            all_article_urls.extend(article_urls)\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "    return list(set(all_article_urls))\n",
    "\n",
    "# Function to filter articles by date\n",
    "def filter_by_date(date_str, start_date):\n",
    "    try:\n",
    "        article_date = datetime.strptime(date_str, '%B %d, %Y')\n",
    "        return article_date >= start_date\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Scrape articles from the sports section\n",
    "def scrape_section(entry):\n",
    "    base_url = f\"https://dailynexus.com/category/{entry}/\"\n",
    "    \n",
    "    article_urls = get_all_article_urls(base_url)\n",
    "    print(f\"Found {len(article_urls)} articles in section {entry}.\")\n",
    "\n",
    "    start_date = datetime(2018, 1, 1)\n",
    "    articles = []\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    # Process each article and collect data\n",
    "    for article_url in article_urls:\n",
    "        article_html = fetch_html(article_url)\n",
    "        if article_html:\n",
    "            title, date, time, authors, content = extract_article_content(article_html)\n",
    "            if filter_by_date(date, start_date):\n",
    "                articles.append({\n",
    "                    'Section': entry,\n",
    "                    'Title': title[0:-18],\n",
    "                    'Date': date,\n",
    "                    'Time': time,\n",
    "                    'Authors': authors,\n",
    "                    'URL': article_url,\n",
    "                    'Content': content\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Failed to retrieve article: {article_url}\")\n",
    "        print(count)\n",
    "        count += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Define the header for the CSV file\n",
    "header = ['Section', 'Title', 'Date', 'Time', 'Authors', 'URL', 'Content']\n",
    "\n",
    "# Scrape the sports section and collect all data\n",
    "articles = scrape_section('la-vista')\n",
    "\n",
    "print(f\"Scraping completed. Total articles written: {len(articles)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(articles).to_csv('la-vista.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Missed Articles (Only if Missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Scraping completed. Total articles written: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the keywords of interest\n",
    "keywords = [\n",
    "    \"asian\", \"hispanic\", \"latinx\", \"diversity\", \"equity\", \"inclusion\", \"equality\", \n",
    "    \"poc\", \"lgbtqia\", \"queer\", \"middle eastern\", \"black\", \"gay\", \"transgender\", \n",
    "    \"bisexual\", \"lesbian\", \"pansexual\", \"asexual\", \"homosexual\", \"nonbinary\", \n",
    "    \"intersex\", \"aromantic\", \"cisgender\", \"coming out\", \"genderfluid\", \"privilege\", \n",
    "    \"pronouns\", \"undocumented\", \"neurodiversity\", \"neurodivergent\", \"disability\", \n",
    "    \"microaggression\", \"identity\", \"discrimination\", \"culture\", \"misgendering\"\n",
    "]\n",
    "\n",
    "# Function to fetch the HTML content of a URL\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract article URLs from a page\n",
    "def extract_article_urls(page_html):\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    article_tags = soup.find_all('div', class_='category-page-post-text')\n",
    "    article_urls = [tag.find('a')['href'] for tag in article_tags if tag.find('a')]\n",
    "    return list(set(article_urls))\n",
    "\n",
    "def clean_content(content):\n",
    "    marker = \"Print Comments are closed.\"\n",
    "    if (idx := content.find(marker)) != -1:\n",
    "        content = content[:idx]\n",
    "    \n",
    "    # Replace \"â€œ\" with opening quotes and \"â€\" with closing quotes\n",
    "    content = content.replace(\"â€œ\", \"\\\"\").replace(\"â€\", \"\\\"\").replace(\"â€™\", \"\\'\")\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Function to extract article content\n",
    "def extract_article_content(article_html):\n",
    "    soup = BeautifulSoup(article_html, 'html.parser')\n",
    "    title_tag = soup.find('title')\n",
    "    date_tag = soup.find('div', class_='single-post-byline')\n",
    "    title = title_tag.text.strip() if title_tag else 'No title found'\n",
    "    date_info = date_tag.text.strip() if date_tag else 'No date found'\n",
    "    \n",
    "    date, time, authors = parse_date_info(date_info)\n",
    "    \n",
    "    content = ' '.join([p.text for p in soup.find_all('p')])\n",
    "    content = clean_content(content)\n",
    "    return title, date, time, authors, content\n",
    "\n",
    "# Function to parse date information\n",
    "def parse_date_info(date_info):\n",
    "    date_time_author_pattern = re.compile(r\"([A-Za-z]+\\s\\d{1,2},\\s\\d{4})\\s+at\\s+(\\d{1,2}:\\d{2}\\s[apm]{2})\\s+by\\s+(.+)\")\n",
    "    match = date_time_author_pattern.search(date_info)\n",
    "    \n",
    "    if match:\n",
    "        date = match.group(1)\n",
    "        time = match.group(2)\n",
    "        authors_str = match.group(3)\n",
    "        authors = [author.strip() for author in re.split(r',\\s*|\\band\\b', authors_str)]\n",
    "        return date, time, authors\n",
    "    else:\n",
    "        return 'No date found', 'No time found', ['No author found']\n",
    "\n",
    "# Function to check if the article content contains any of the keywords\n",
    "def contains_keywords(content, keywords):\n",
    "    content_lower = content.lower()\n",
    "    for keyword in keywords:\n",
    "        if re.search(r'\\b' + re.escape(keyword.lower()) + r'\\b', content_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to get all article URLs by paginating\n",
    "def get_all_article_urls(base_url, max_pages=161):\n",
    "    page = 1\n",
    "    all_article_urls = []\n",
    "    while page <= max_pages:\n",
    "        page_url = f\"{base_url}page/{page}/\"\n",
    "        if page % 10 == 0:\n",
    "            print(f\"Fetching {page_url}...\")\n",
    "        page_html = fetch_html(page_url)\n",
    "        if page_html:\n",
    "            article_urls = extract_article_urls(page_html)\n",
    "            if not article_urls:\n",
    "                break  # No more articles found, exit the loop\n",
    "            all_article_urls.extend(article_urls)\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "    return list(set(all_article_urls))\n",
    "\n",
    "# Function to filter articles by date\n",
    "def filter_by_date(date_str, start_date):\n",
    "    try:\n",
    "        article_date = datetime.strptime(date_str, '%B %d, %Y')\n",
    "        return article_date >= start_date\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Scrape articles from the sports section\n",
    "def scrape_section(entry):\n",
    "    base_url = f\"https://dailynexus.com/category/{entry}/\"\n",
    "    \n",
    "    article_urls = ['https://dailynexus.com/2021-03-11/report-it-rained/']\n",
    "\n",
    "    start_date = datetime(2018, 1, 1)\n",
    "    articles = []\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    # Process each article and collect data\n",
    "    for article_url in article_urls:\n",
    "        article_html = fetch_html(article_url)\n",
    "        if article_html:\n",
    "            title, date, time, authors, content = extract_article_content(article_html)\n",
    "            if filter_by_date(date, start_date):\n",
    "                articles.append({\n",
    "                    'Section': entry,\n",
    "                    'Title': title[0:-18],\n",
    "                    'Date': date,\n",
    "                    'Time': time,\n",
    "                    'Authors': authors,\n",
    "                    'URL': article_url,\n",
    "                    'Content': content\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Failed to retrieve article: {article_url}\")\n",
    "        print(count)\n",
    "        count += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Define the header for the CSV file\n",
    "header = ['Section', 'Title', 'Date', 'Time', 'Authors', 'URL', 'Content']\n",
    "\n",
    "# Scrape the section and collect all data\n",
    "articles = scrape_section('daily-stench')\n",
    "\n",
    "print(f\"Scraping completed. Total articles written: {len(articles)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append entry in articles to section.csv\n",
    "with open('daily-stench.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    for article in articles:\n",
    "        writer.writerow(article)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
